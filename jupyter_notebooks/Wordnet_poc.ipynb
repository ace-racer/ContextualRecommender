{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms\n",
      "{'thoroughly', 'dear', 'estimable', 'unspoiled', 'unspoilt', 'skillful', 'honest', 'honorable', 'adept', 'serious', 'just', 'salutary', 'expert', 'in_force', 'secure', 'practiced', 'effective', 'in_effect', 'full', 'respectable', 'goodness', 'good', 'well', 'sound', 'dependable', 'safe', 'skilful', 'commodity', 'proficient', 'beneficial', 'ripe', 'undecomposed', 'upright', 'near', 'trade_good', 'right', 'soundly'}\n",
      "{'badness', 'bad', 'evilness', 'ill', 'evil'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(\"Synonyms\")\n",
    "print(set(synonyms))\n",
    "\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good.n.01\n",
      "nice.n.01\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"good\")\n",
    "print(syns[0].name())\n",
    "syns_1 = wordnet.synsets(\"nice\")\n",
    "print(syns_1[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benefit\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(syns[0].name())\n",
    "w2 = wordnet.synset(syns_1[0].name())\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream_details():\n",
    "    print(\"Reading the stream details...\")\n",
    "    complete_stream_details_df = pd.read_csv(\"/Users/shriyashekhar/Documents/IWork/Data/tag_generation_content/card_module_details_content_extracted.csv\", encoding=\"ISO-8859-1\")\n",
    "    if complete_stream_details_df is not None:\n",
    "        complete_stream_details_dict = {}\n",
    "        _stream_id_stream_title_dict = {}\n",
    "        for _, row in complete_stream_details_df.iterrows():\n",
    "            \n",
    "            stream_id = str(row[\"DECKID\"])\n",
    "            stream_title = str(row[\"DECKNAME\"])\n",
    "            row_content = str(row[\"HTML_CONTENT\"])\n",
    "\n",
    "            # TODO: add the card title and the module name to the content on which the tags can be generated\n",
    "            card_title =str(row[\"CARDTITLE\"])\n",
    "            module_name = str(row[\"MODULENAME\"])\n",
    "            \n",
    "            if row_content and \"nan\" not in row_content:\n",
    "                # if the stream ID already exists in the dictionary\n",
    "                if complete_stream_details_dict.get(stream_id):\n",
    "                    existing_content = complete_stream_details_dict[stream_id]\n",
    "                    new_content = existing_content + \"\\n\" + row_content.strip()\n",
    "                    complete_stream_details_dict[stream_id] = new_content\n",
    "                else:\n",
    "                    complete_stream_details_dict[stream_id] = row_content.strip()\n",
    "                    _stream_id_stream_title_dict[stream_id] = stream_title\n",
    "        \n",
    "        return complete_stream_details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the stream details...\n"
     ]
    }
   ],
   "source": [
    "stream_details_dict = get_stream_details()\n",
    "#stream_details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StreamID</th>\n",
       "      <th>Content</th>\n",
       "      <th>ContentLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163</td>\n",
       "      <td>TXmAk2KZAy4\\nNMeUjebo1Ac\\nEEuTxFhp3go\\nCastrol...</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>419</td>\n",
       "      <td>TXmAk2KZAy4\\nNMeUjebo1Ac\\nEEuTxFhp3go\\nCastrol...</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507</td>\n",
       "      <td>wBYKUgUyGWc\\nA team of world-class drivers, po...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>Castrol EDGE  is Castrols flagship power bran...</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201</td>\n",
       "      <td>Charles Cheers Wakefield, Castrols founder,...</td>\n",
       "      <td>1662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StreamID                                            Content  ContentLength\n",
       "0      163  TXmAk2KZAy4\\nNMeUjebo1Ac\\nEEuTxFhp3go\\nCastrol...            801\n",
       "1      419  TXmAk2KZAy4\\nNMeUjebo1Ac\\nEEuTxFhp3go\\nCastrol...            813\n",
       "2      507  wBYKUgUyGWc\\nA team of world-class drivers, po...            181\n",
       "3      199  Castrol EDGE  is Castrols flagship power bran...           2550\n",
       "4      201  Charles Cheers Wakefield, Castrols founder,...           1662"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the content length\n",
    "stream_details_df = pd.DataFrame(list(stream_details_dict.items()), columns = [\"StreamID\", \"Content\"])\n",
    "stream_details_df[\"ContentLength\"] = stream_details_df[\"Content\"].str.len()\n",
    "stream_details_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "# convert case, remove punctuations\n",
    "# stem the words\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "stop = stopwords.words('english')\n",
    "snowball = nltk.SnowballStemmer('english')\n",
    "\n",
    "def preprocess(toks):\n",
    "    toks = [ t.lower() for t in toks if t not in string.punctuation ]\n",
    "    toks = [t for t in toks if t not in stop ]\n",
    "    toks = [ snowball.stem(t) for t in toks ]\n",
    "#   toks = [ wnl.lemmatize(t) for t in toks ]\n",
    "    toks_clean = [ t for t in toks if len(t) >= 3 ]\n",
    "    return toks_clean\n",
    "\n",
    "all_streams_cleaned_tokens = []\n",
    "stream_ids = list(stream_details_df[\"StreamID\"])\n",
    "i=0\n",
    "combined = []\n",
    "for _, row in stream_details_df.iterrows():\n",
    "    stream_content = row[\"Content\"]\n",
    "    stream_content_tokens = nltk.word_tokenize(stream_content)\n",
    "    stream_cleaned_tokens = preprocess(stream_content_tokens)\n",
    "    all_streams_cleaned_tokens.append(stream_cleaned_tokens)\n",
    "#     print(stream_cleaned_tokens)\n",
    "    n_grams = ngrams(stream_cleaned_tokens, 2)\n",
    "    #num_ngrams = len(list(n_grams))\n",
    "    print(stream_cleaned_token)\n",
    "    #combined = \"_\".join(list(n_grams))\n",
    "#     print(list(n_grams))\n",
    "    for stream_cleaned_token in stream_cleaned_tokens:\n",
    "        for syn in wordnet.synsets(stream_cleaned_token):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "#                 print(\"Synonyms of\")\n",
    "#                 print(set(synonyms))\n",
    "    \n",
    "    for n_gram in n_grams:\n",
    "        combined_val = \"_\".join(n_gram)\n",
    "        combined.append(combined_val)\n",
    "        print(combined)\n",
    "    #i+=1\n",
    "#     for grams in n_grams:\n",
    "#         print(list(grams))\n",
    "\n",
    "print(\"Original number of 3 tuples: \" + str(len(combined)))\n",
    "# to remove duplicates\n",
    "combined = set(combined)\n",
    "#print(combined)\n",
    "print(\"After duplicate removal: \" + str(len(combined)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
